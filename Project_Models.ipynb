{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout\n",
    "import joblib\n",
    "import warnings\n",
    "# Suppress all warnings for a cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "#import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    #for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training pipeline with hyperparameter tuning...\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting model training pipeline with hyperparameter tuning...\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(r\"C:\\Users\\linae\\OneDrive\\Desktop\\Northwestern\\MSDS 422\\Project\\credit_risk_dataset.csv\")\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'credit_risk_dataset.csv' not found. Please ensure the file path is correct.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling missing values...\n",
      "Initial number of rows: 32581\n",
      "Number of rows after dropping NaNs: 28638 (3943 rows dropped)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values in 'person_emp_length' and 'loan_int_rate'\n",
    "print(\"\\nHandling missing values...\")\n",
    "initial_rows = df.shape[0]\n",
    "df_clean = df.dropna().copy()\n",
    "rows_dropped = initial_rows - df_clean.shape[0]\n",
    "print(f\"Initial number of rows: {initial_rows}\")\n",
    "print(f\"Number of rows after dropping NaNs: {df_clean.shape[0]} ({rows_dropped} rows dropped)\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing one-hot encoding on categorical features...\n",
      "Categorical features encoded successfully.\n",
      "New DataFrame shape: (28638, 23)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerforming one-hot encoding on categorical features...\")\n",
    "categorical_cols = [\n",
    "    'person_home_ownership',\n",
    "    'loan_intent',\n",
    "    'loan_grade',\n",
    "    'cb_person_default_on_file'\n",
    "]\n",
    "df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
    "print(\"Categorical features encoded successfully.\")\n",
    "print(f\"New DataFrame shape: {df_encoded.shape}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data into training and testing sets...\n",
      "Training set size: 22910\n",
      "Testing set size: 5728\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) from the target variable (y)\n",
    "print(\"\\nSplitting data into training and testing sets...\")\n",
    "X = df_encoded.drop('loan_status', axis=1)\n",
    "y = df_encoded['loan_status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaling numerical features...\n",
      "Features scaled successfully.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nScaling numerical features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Features scaled successfully.\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and tuning Logistic Regression model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining and tuning Logistic Regression model...\")\n",
    "# Define the hyperparameters to search\n",
    "log_reg_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "# Initialize GridSearchCV with the model and parameter grid\n",
    "log_reg_grid_search = GridSearchCV(\n",
    "    LogisticRegression(solver='liblinear', random_state=42),\n",
    "    log_reg_param_grid,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "log_reg_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from the search\n",
    "best_log_reg_model = log_reg_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "y_pred_log_reg = best_log_reg_model.predict(X_test_scaled)\n",
    "y_pred_proba_log_reg = best_log_reg_model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Best Logistic Regression Model Evaluation ---\n",
      "Best Parameters: {'C': 0.1, 'penalty': 'l1'}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91      4487\n",
      "           1       0.75      0.53      0.62      1241\n",
      "\n",
      "    accuracy                           0.86      5728\n",
      "   macro avg       0.82      0.74      0.77      5728\n",
      "weighted avg       0.85      0.86      0.85      5728\n",
      "\n",
      "ROC-AUC Score: 0.8617\n",
      "----------------------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[4272  215]\n",
      " [ 588  653]]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logistic_regression_model.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "print(\"\\n--- Best Logistic Regression Model Evaluation ---\")\n",
    "print(f\"Best Parameters: {log_reg_grid_search.best_params_}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_log_reg):.4f}\")\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg, pos_label=1)\n",
    "print(\"-\" * 70)\n",
    "cm = confusion_matrix(y_test, y_pred_log_reg)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"-\" * 70)\n",
    "joblib.dump(best_log_reg_model, 'logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best Logistic Regression model and metadata to disk.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Save model metadata\n",
    "log_reg_metadata = {\n",
    "    'model_name': 'Logistic Regression',\n",
    "    'best_parameters': log_reg_grid_search.best_params_,\n",
    "    'roc_auc_score': roc_auc_score,\n",
    "    'f1_score_positive_class': f1_log_reg\n",
    "}\n",
    "joblib.dump(log_reg_metadata, 'logistic_regression_metadata.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Saved best Logistic Regression model and metadata to disk.\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "### Why This Model Was Used\n",
    "Logistic Regression is a foundational and highly interpretable machine learning algorithm. It was included in the project for two main reasons:\n",
    "\n",
    "Interpretability: Its linear nature makes it easy to understand which features are most important and how they influence the final prediction. This is critical for model explainability and regulatory compliance, as mentioned in your abstract.\n",
    "\n",
    "Baseline Performance: As a \"traditional\" statistical model, it provides a solid baseline to compare the performance of more complex machine learning models like XGBoost and LightGBM. If a complex model doesn't significantly outperform the simple one, it may not be worth the added complexity.\n",
    "\n",
    "### How It Was Trained\n",
    "The model was trained using GridSearchCV on the scaled data. Scaling is crucial for Logistic Regression because it is sensitive to the magnitude of the features. Without scaling, a feature with a large range (like person_income) could dominate the model's training over a feature with a small range (like loan_grade). The GridSearchCV method systematically tested different combinations of hyperparameters to find the best performing model.\n",
    "\n",
    "### Key Parameters Used\n",
    "C: This is the regularization parameter. It controls how much the model's complexity is penalized. A smaller C value increases the penalty, which helps prevent overfitting by making the model simpler.\n",
    "\n",
    "penalty: This specifies the type of regularization used. l1 regularization shrinks the coefficients of less important features to zero, effectively performing feature selection. l2 regularization (which is the default) shrinks the coefficients but doesn't force them to be exactly zero.\n",
    "\n",
    "solver='liblinear': A specific algorithm used to optimize the model. It is an efficient choice for smaller datasets and for when you want to use l1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and tuning XGBoost model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining and tuning XGBoost model...\")\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    }
   ],
   "source": [
    "xgb_grid_search = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    xgb_param_grid,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the search\n",
    "best_xgb_model = xgb_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "y_pred_xgb = best_xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = best_xgb_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Best XGBoost Model Evaluation ---\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      4487\n",
      "           1       0.97      0.72      0.82      1241\n",
      "\n",
      "    accuracy                           0.93      5728\n",
      "   macro avg       0.95      0.85      0.89      5728\n",
      "weighted avg       0.94      0.93      0.93      5728\n",
      "\n",
      "ROC-AUC Score: 0.9429\n",
      "Confusion Matrix:\n",
      "[[4272  215]\n",
      " [ 588  653]]\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_model.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- Best XGBoost Model Evaluation ---\")\n",
    "print(f\"Best Parameters: {xgb_grid_search.best_params_}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
    "f1_xgb = f1_score(y_test, y_pred_log_reg, pos_label=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_log_reg)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"-\" * 70)\n",
    "print(\"-\" * 70)\n",
    "joblib.dump(best_xgb_model, 'xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best XGBoost model and metadata to disk.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "xgb_metadata = {\n",
    "    'model_name': 'XGBoost',\n",
    "    'best_parameters': xgb_grid_search.best_params_,\n",
    "    'roc_auc_score': roc_auc_score,\n",
    "    'f1_score_positive_class': f1_xgb\n",
    "}\n",
    "joblib.dump(xgb_metadata, 'xgboost_metadata.pkl')\n",
    "print(\"Saved best XGBoost model and metadata to disk.\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model\n",
    "### Why This Model Was Used\n",
    "XGBoost (eXtreme Gradient Boosting) is one of the most powerful and widely used machine learning algorithms for tabular data. It was a perfect fit for this project because it is known for its high predictive accuracy and computational efficiency, which are essential for building a robust credit risk scoring system. It excels at capturing complex, non-linear relationships in the data.\n",
    "\n",
    "### How It Was Trained\n",
    "The model was trained using GridSearchCV on the unscaled data. Gradient boosting models like XGBoost are tree-based, so they are not sensitive to the scale of the features. This makes them less reliant on a separate scaling step. GridSearchCV was used to find the optimal combination of hyperparameters that would result in the highest ROC-AUC score.\n",
    "\n",
    "### Key Parameters Used\n",
    "n_estimators: This is the number of boosting rounds or the number of decision trees to be built sequentially. More trees can improve performance but also increase the risk of overfitting and training time.\n",
    "\n",
    "max_depth: This controls the maximum depth of each tree. A deeper tree can capture more complex interactions but may also lead to overfitting.\n",
    "\n",
    "learning_rate: This determines the step size at each boosting iteration. A lower learning rate requires more trees but makes the model more robust to overfitting. The ideal learning_rate is usually a trade-off between model performance and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and tuning SVM model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining and tuning SVM model\")\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['rbf']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "# Initialize GridSearchCV\n",
    "svm_grid_search = GridSearchCV(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    svm_param_grid,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "svm_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from the search\n",
    "best_svm_model = svm_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "y_pred_svm = best_svm_model.predict(X_test_scaled)\n",
    "y_pred_proba_svm = best_svm_model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Best SVM Model Evaluation ---\n",
      "Best Parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94      4487\n",
      "           1       0.93      0.62      0.74      1241\n",
      "\n",
      "    accuracy                           0.91      5728\n",
      "   macro avg       0.91      0.80      0.84      5728\n",
      "weighted avg       0.91      0.91      0.90      5728\n",
      "\n",
      "ROC-AUC Score: 0.8893\n",
      "Confusion Matrix:\n",
      "[[4272  215]\n",
      " [ 588  653]]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- Best SVM Model Evaluation ---\")\n",
    "print(f\"Best Parameters: {svm_grid_search.best_params_}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_svm):.4f}\")\n",
    "f1_svm = f1_score(y_test, y_pred_log_reg, pos_label=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_log_reg)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"-\" * 70)\n",
    "joblib.dump(best_svm_model, 'svm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best SVM model and metadata to disk.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "svm_metadata = {\n",
    "    'model_name': 'Support Vector Machine',\n",
    "    'best_parameters': svm_grid_search.best_params_,\n",
    "    'roc_auc_score': roc_auc_score,\n",
    "    'f1_score_positive_class': f1_svm\n",
    "}\n",
    "joblib.dump(svm_metadata, 'svm_metadata.pkl')\n",
    "print(\"Saved best SVM model and metadata to disk.\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) Model\n",
    "### Why This Model Was Used\n",
    "SVM is a powerful model for both linear and non-linear classification. It was included to demonstrate the project's capability to handle a variety of algorithms, including those that are not tree-based. It is known for its effectiveness in high-dimensional spaces and for its ability to model complex decision boundaries.\n",
    "\n",
    "### How It Was Trained\n",
    "Like Logistic Regression, the SVM model was trained using GridSearchCV on the scaled data. SVM's performance is highly dependent on feature scaling because the algorithm calculates distances between data points. The GridSearchCV was used to optimize for the best C, gamma, and kernel combination.\n",
    "\n",
    "### Key Parameters Used\n",
    "C: The regularization parameter. It controls the trade-off between a smooth decision boundary and correctly classifying training points. A smaller C emphasizes a wider margin (more simple boundary) while a larger C aims for a more precise classification of each training point, which can lead to overfitting.\n",
    "\n",
    "gamma: The kernel coefficient for the rbf (Radial Basis Function) kernel. It defines how far the influence of a single training example reaches. A small gamma means a large influence, and a large gamma means a small influence.\n",
    "\n",
    "kernel: The kernel function specifies the type of transformation applied to the data. We used the 'rbf' kernel, which is highly effective for non-linear problems and is what allows the SVM to find complex, curved decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and tuning LightGBM model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining and tuning LightGBM model...\")\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'num_leaves': [31, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4962, number of negative: 17948\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 923\n",
      "[LightGBM] [Info] Number of data points in the train set: 22910, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.216587 -> initscore=-1.285670\n",
      "[LightGBM] [Info] Start training from score -1.285670\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "lgb_grid_search = GridSearchCV(\n",
    "    lgb.LGBMClassifier(random_state=42),\n",
    "    lgb_param_grid,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_lgb_model = lgb_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgb = best_lgb_model.predict(X_test)\n",
    "y_pred_proba_lgb = best_lgb_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Best LightGBM Model Evaluation ---\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'num_leaves': 50}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      4487\n",
      "           1       0.95      0.72      0.82      1241\n",
      "\n",
      "    accuracy                           0.93      5728\n",
      "   macro avg       0.94      0.86      0.89      5728\n",
      "weighted avg       0.93      0.93      0.93      5728\n",
      "\n",
      "ROC-AUC Score: 0.9434\n",
      "Confusion Matrix:\n",
      "[[4272  215]\n",
      " [ 588  653]]\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lightgbm_model.pkl']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- Best LightGBM Model Evaluation ---\")\n",
    "print(f\"Best Parameters: {lgb_grid_search.best_params_}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lgb))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_lgb):.4f}\")\n",
    "f1_lgb = f1_score(y_test, y_pred_log_reg, pos_label=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_log_reg)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"-\" * 70)\n",
    "joblib.dump(best_lgb_model, 'lightgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best LightGBM model and metadata to disk.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lgb_metadata = {\n",
    "    'model_name': 'LightGBM',\n",
    "    'best_parameters': lgb_grid_search.best_params_,\n",
    "    'roc_auc_score': roc_auc_score,\n",
    "    'f1_score_positive_class': f1_lgb\n",
    "}\n",
    "joblib.dump(lgb_metadata, 'lightgbm_metadata.pkl')\n",
    "print(\"Saved best LightGBM model and metadata to disk.\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Model\n",
    "## Why This Model Was Used\n",
    "LightGBM is a gradient boosting framework that is designed to be highly efficient and fast. It was chosen as a primary model because it is often faster than XGBoost and can handle large datasets with high performance. Its unique tree-growing strategy (leaf-wise vs. level-wise) makes it a strong competitor for real-time applications.\n",
    "\n",
    "## How It Was Trained\n",
    "Similar to XGBoost, LightGBM is a tree-based model and was trained using GridSearchCV on the unscaled data. This makes it less sensitive to feature scaling. The grid search was used to tune its key hyperparameters to maximize the ROC-AUC score.\n",
    "\n",
    "## Key Parameters Used\n",
    "n_estimators: The number of boosting rounds or trees, similar to XGBoost.\n",
    "\n",
    "max_depth: The maximum depth of each tree.\n",
    "\n",
    "learning_rate: The step size shrinkage to prevent overfitting.\n",
    "\n",
    "num_leaves: This is a unique parameter for LightGBM. It controls the maximum number of leaves in one tree. A larger number of leaves can increase the model's complexity and accuracy, but also its risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 1D CNN model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining 1D CNN model...\")\n",
    "# Reshape the data for CNN input (samples, timesteps, features)\n",
    "X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the 1D CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=(1, X_train_scaled.shape[1])),\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - AUC: 0.8108 - accuracy: 0.8334 - loss: 0.4074 - val_AUC: 0.8909 - val_accuracy: 0.8865 - val_loss: 0.3175\n",
      "Epoch 2/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - AUC: 0.8654 - accuracy: 0.8679 - loss: 0.3431 - val_AUC: 0.8974 - val_accuracy: 0.8942 - val_loss: 0.3018\n",
      "Epoch 3/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - AUC: 0.8749 - accuracy: 0.8790 - loss: 0.3261 - val_AUC: 0.9013 - val_accuracy: 0.8990 - val_loss: 0.2893\n",
      "Epoch 4/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - AUC: 0.8809 - accuracy: 0.8862 - loss: 0.3110 - val_AUC: 0.9042 - val_accuracy: 0.9020 - val_loss: 0.2832\n",
      "Epoch 5/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - AUC: 0.8859 - accuracy: 0.8898 - loss: 0.3028 - val_AUC: 0.9087 - val_accuracy: 0.9042 - val_loss: 0.2762\n",
      "Epoch 6/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - AUC: 0.8884 - accuracy: 0.8949 - loss: 0.2951 - val_AUC: 0.9104 - val_accuracy: 0.9075 - val_loss: 0.2712\n",
      "Epoch 7/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.8946 - accuracy: 0.8989 - loss: 0.2860 - val_AUC: 0.9121 - val_accuracy: 0.9088 - val_loss: 0.2673\n",
      "Epoch 8/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - AUC: 0.8961 - accuracy: 0.9019 - loss: 0.2825 - val_AUC: 0.9146 - val_accuracy: 0.9101 - val_loss: 0.2662\n",
      "Epoch 9/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - AUC: 0.8969 - accuracy: 0.9038 - loss: 0.2783 - val_AUC: 0.9156 - val_accuracy: 0.9123 - val_loss: 0.2640\n",
      "Epoch 10/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - AUC: 0.8984 - accuracy: 0.9039 - loss: 0.2720 - val_AUC: 0.9187 - val_accuracy: 0.9142 - val_loss: 0.2613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17589c62540>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC', 'accuracy'])\n",
    "# Train the model\n",
    "cnn_model.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "cnn_eval = cnn_model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "y_pred_proba_cnn = cnn_model.predict(X_test_cnn)\n",
    "y_pred_cnn = (y_pred_proba_cnn > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1D CNN Model Evaluation ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94      4487\n",
      "           1       0.94      0.62      0.74      1241\n",
      "\n",
      "    accuracy                           0.91      5728\n",
      "   macro avg       0.92      0.80      0.84      5728\n",
      "weighted avg       0.91      0.91      0.90      5728\n",
      "\n",
      "ROC-AUC Score: 0.9113\n",
      "Confusion Matrix:\n",
      "[[4272  215]\n",
      " [ 588  653]]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 1D CNN Model Evaluation ---\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_cnn))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_cnn):.4f}\")\n",
    "f1_cnn = f1_score(y_test, y_pred_log_reg, pos_label=1)\n",
    "cm = confusion_matrix(y_test, y_pred_log_reg)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"-\" * 70)\n",
    "cnn_model.save('cnn_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,472</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m1,472\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,765</span> (116.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,765\u001b[0m (116.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,921</span> (38.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,921\u001b[0m (38.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,844</span> (77.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m19,844\u001b[0m (77.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'roc_auc_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save model metadata\u001b[39;00m\n\u001b[0;32m      2\u001b[0m cnn_metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1D CNN\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m: cnn_model\u001b[38;5;241m.\u001b[39msummary(),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc_score\u001b[39m\u001b[38;5;124m'\u001b[39m: roc_auc_cnn,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score_positive_class\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_cnn\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     11\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(cnn_metadata, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn_metadata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved 1D CNN model and metadata to disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'roc_auc_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "# Save model metadata\n",
    "cnn_metadata = {\n",
    "    'model_name': '1D CNN',\n",
    "    'architecture': cnn_model.summary(),\n",
    "    'training_epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'optimizer': 'adam',\n",
    "    'roc_auc_score': roc_auc_score,\n",
    "    'f1_score_positive_class': f1_cnn\n",
    "}\n",
    "joblib.dump(cnn_metadata, 'cnn_metadata.pkl')\n",
    "\n",
    "print(\"Saved 1D CNN model and metadata to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN Model\n",
    "### Why This Model Was Used\n",
    "A 1D CNN was included to explore a deep learning approach to the problem. While CNNs are most famous for image processing (where they use 2D or 3D convolutions), a 1D CNN is effective for tabular data and sequential data. It can automatically learn and extract important features from the input data, potentially discovering patterns that other models might miss.\n",
    "\n",
    "### How It Was Trained\n",
    "This model was built using Sequential layers from Keras. Unlike the other models, it was not trained with GridSearchCV. Instead, it was trained using the fit() method over a fixed number of epochs. It was trained on the scaled data, as neural networks perform better when features are on a similar scale.\n",
    "\n",
    "### Key Parameters Used\n",
    "Conv1D: This is the core layer. It applies a 1D convolution operation to the input data, which helps to automatically learn important features from the columns.\n",
    "\n",
    "Dropout: This layer randomly sets a fraction of input units to 0 at each update during training. This is a powerful technique for preventing overfitting by ensuring the network doesn't become too reliant on a small number of features.\n",
    "\n",
    "Dense: These are standard fully-connected layers. They take all inputs from the previous layer and connect them to every neuron in the current layer.\n",
    "\n",
    "optimizer='adam': The optimization algorithm used to train the network. Adam is a popular and efficient choice that adapts the learning rate for each parameter.\n",
    "\n",
    "epochs=10: The number of times the model will go through the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel training pipeline complete. All three models and the scaler have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# files_to_remove = [\n",
    "#     '/kaggle/working/output_file1.csv',\n",
    "#     '/kaggle/working/output_file2.csv',\n",
    "#     '/kaggle/working/output_file3.csv'\n",
    "# ]\n",
    "\n",
    "# for file_path in files_to_remove:\n",
    "#     if os.path.exists(file_path):\n",
    "#         os.remove(file_path)\n",
    "#         print(f\"Removed: {file_path}\")\n",
    "#     else:\n",
    "#         print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.6382326721640295, 1: 2.3085449415558243}\n",
      "Epoch 1/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.7292 - auc: 0.8299 - loss: 0.5313 - pr_auc: 0.6376 - val_accuracy: 0.8385 - val_auc: 0.8831 - val_loss: 0.4801 - val_pr_auc: 0.7612\n",
      "Epoch 2/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8186 - auc: 0.8797 - loss: 0.4327 - pr_auc: 0.7426 - val_accuracy: 0.8499 - val_auc: 0.8955 - val_loss: 0.4131 - val_pr_auc: 0.7816\n",
      "Epoch 3/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8337 - auc: 0.8913 - loss: 0.4077 - pr_auc: 0.7786 - val_accuracy: 0.8527 - val_auc: 0.9018 - val_loss: 0.3730 - val_pr_auc: 0.7950\n",
      "Epoch 4/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8390 - auc: 0.8977 - loss: 0.3953 - pr_auc: 0.7902 - val_accuracy: 0.8652 - val_auc: 0.9055 - val_loss: 0.3427 - val_pr_auc: 0.8063\n",
      "Epoch 5/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8482 - auc: 0.9015 - loss: 0.3861 - pr_auc: 0.8008 - val_accuracy: 0.8623 - val_auc: 0.9066 - val_loss: 0.3441 - val_pr_auc: 0.8127\n",
      "Epoch 6/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8491 - auc: 0.9044 - loss: 0.3795 - pr_auc: 0.8097 - val_accuracy: 0.8713 - val_auc: 0.9087 - val_loss: 0.3310 - val_pr_auc: 0.8160\n",
      "Epoch 7/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8561 - auc: 0.9064 - loss: 0.3741 - pr_auc: 0.8149 - val_accuracy: 0.8766 - val_auc: 0.9105 - val_loss: 0.3250 - val_pr_auc: 0.8237\n",
      "Epoch 8/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8582 - auc: 0.9102 - loss: 0.3664 - pr_auc: 0.8203 - val_accuracy: 0.8757 - val_auc: 0.9102 - val_loss: 0.3261 - val_pr_auc: 0.8223\n",
      "Epoch 9/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8646 - auc: 0.9107 - loss: 0.3634 - pr_auc: 0.8261 - val_accuracy: 0.8799 - val_auc: 0.9134 - val_loss: 0.3209 - val_pr_auc: 0.8252\n",
      "Epoch 10/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8658 - auc: 0.9140 - loss: 0.3576 - pr_auc: 0.8303 - val_accuracy: 0.8921 - val_auc: 0.9134 - val_loss: 0.3077 - val_pr_auc: 0.8343\n",
      "Epoch 11/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8698 - auc: 0.9141 - loss: 0.3546 - pr_auc: 0.8343 - val_accuracy: 0.8890 - val_auc: 0.9145 - val_loss: 0.3073 - val_pr_auc: 0.8355\n",
      "Epoch 12/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8743 - auc: 0.9181 - loss: 0.3472 - pr_auc: 0.8407 - val_accuracy: 0.8916 - val_auc: 0.9148 - val_loss: 0.3007 - val_pr_auc: 0.8381\n",
      "Epoch 13/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8725 - auc: 0.9169 - loss: 0.3481 - pr_auc: 0.8414 - val_accuracy: 0.8879 - val_auc: 0.9165 - val_loss: 0.3098 - val_pr_auc: 0.8411\n",
      "Epoch 14/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8766 - auc: 0.9183 - loss: 0.3435 - pr_auc: 0.8455 - val_accuracy: 0.8961 - val_auc: 0.9157 - val_loss: 0.3019 - val_pr_auc: 0.8422\n",
      "Epoch 15/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8793 - auc: 0.9196 - loss: 0.3418 - pr_auc: 0.8459 - val_accuracy: 0.8905 - val_auc: 0.9154 - val_loss: 0.3120 - val_pr_auc: 0.8414\n",
      "Epoch 16/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8783 - auc: 0.9208 - loss: 0.3379 - pr_auc: 0.8493 - val_accuracy: 0.8940 - val_auc: 0.9172 - val_loss: 0.3031 - val_pr_auc: 0.8466\n",
      "Epoch 17/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8807 - auc: 0.9226 - loss: 0.3357 - pr_auc: 0.8523 - val_accuracy: 0.8946 - val_auc: 0.9189 - val_loss: 0.3018 - val_pr_auc: 0.8505\n",
      "Epoch 18/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8800 - auc: 0.9230 - loss: 0.3333 - pr_auc: 0.8537 - val_accuracy: 0.9001 - val_auc: 0.9201 - val_loss: 0.2929 - val_pr_auc: 0.8518\n",
      "Epoch 19/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8821 - auc: 0.9228 - loss: 0.3340 - pr_auc: 0.8529 - val_accuracy: 0.8986 - val_auc: 0.9179 - val_loss: 0.3011 - val_pr_auc: 0.8517\n",
      "Epoch 20/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8830 - auc: 0.9243 - loss: 0.3301 - pr_auc: 0.8569 - val_accuracy: 0.8966 - val_auc: 0.9195 - val_loss: 0.2941 - val_pr_auc: 0.8537\n",
      "Epoch 21/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8822 - auc: 0.9247 - loss: 0.3293 - pr_auc: 0.8562 - val_accuracy: 0.8972 - val_auc: 0.9179 - val_loss: 0.2941 - val_pr_auc: 0.8523\n",
      "Epoch 22/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8830 - auc: 0.9251 - loss: 0.3276 - pr_auc: 0.8580 - val_accuracy: 0.8968 - val_auc: 0.9214 - val_loss: 0.2948 - val_pr_auc: 0.8565\n",
      "Epoch 23/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8818 - auc: 0.9258 - loss: 0.3264 - pr_auc: 0.8580 - val_accuracy: 0.8963 - val_auc: 0.9188 - val_loss: 0.2958 - val_pr_auc: 0.8529\n",
      "Epoch 24/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8886 - auc: 0.9269 - loss: 0.3228 - pr_auc: 0.8625 - val_accuracy: 0.9022 - val_auc: 0.9198 - val_loss: 0.2871 - val_pr_auc: 0.8548\n",
      "Epoch 25/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8842 - auc: 0.9258 - loss: 0.3244 - pr_auc: 0.8612 - val_accuracy: 0.9008 - val_auc: 0.9198 - val_loss: 0.2865 - val_pr_auc: 0.8564\n",
      "Epoch 26/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8881 - auc: 0.9271 - loss: 0.3206 - pr_auc: 0.8640 - val_accuracy: 0.8986 - val_auc: 0.9196 - val_loss: 0.2931 - val_pr_auc: 0.8571\n",
      "Epoch 27/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8871 - auc: 0.9268 - loss: 0.3202 - pr_auc: 0.8629 - val_accuracy: 0.9000 - val_auc: 0.9200 - val_loss: 0.2902 - val_pr_auc: 0.8574\n",
      "Epoch 28/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8884 - auc: 0.9284 - loss: 0.3190 - pr_auc: 0.8653 - val_accuracy: 0.8919 - val_auc: 0.9201 - val_loss: 0.2970 - val_pr_auc: 0.8559\n",
      "Epoch 29/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8919 - auc: 0.9293 - loss: 0.3180 - pr_auc: 0.8666 - val_accuracy: 0.9007 - val_auc: 0.9192 - val_loss: 0.2839 - val_pr_auc: 0.8563\n",
      "Epoch 30/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8860 - auc: 0.9274 - loss: 0.3188 - pr_auc: 0.8657 - val_accuracy: 0.9035 - val_auc: 0.9176 - val_loss: 0.2869 - val_pr_auc: 0.8546\n",
      "Epoch 31/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8929 - auc: 0.9301 - loss: 0.3143 - pr_auc: 0.8698 - val_accuracy: 0.9042 - val_auc: 0.9186 - val_loss: 0.2893 - val_pr_auc: 0.8581\n",
      "Epoch 32/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8918 - auc: 0.9320 - loss: 0.3113 - pr_auc: 0.8702 - val_accuracy: 0.8951 - val_auc: 0.9197 - val_loss: 0.2901 - val_pr_auc: 0.8601\n",
      "Epoch 33/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8936 - auc: 0.9318 - loss: 0.3108 - pr_auc: 0.8715 - val_accuracy: 0.8958 - val_auc: 0.9201 - val_loss: 0.2950 - val_pr_auc: 0.8585\n",
      "Epoch 34/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8883 - auc: 0.9313 - loss: 0.3120 - pr_auc: 0.8701 - val_accuracy: 0.9057 - val_auc: 0.9203 - val_loss: 0.2815 - val_pr_auc: 0.8610\n",
      "Epoch 35/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8911 - auc: 0.9306 - loss: 0.3127 - pr_auc: 0.8685 - val_accuracy: 0.9069 - val_auc: 0.9195 - val_loss: 0.2827 - val_pr_auc: 0.8595\n",
      "Epoch 36/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8910 - auc: 0.9302 - loss: 0.3135 - pr_auc: 0.8680 - val_accuracy: 0.9066 - val_auc: 0.9189 - val_loss: 0.2845 - val_pr_auc: 0.8601\n",
      "Epoch 37/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8927 - auc: 0.9294 - loss: 0.3145 - pr_auc: 0.8696 - val_accuracy: 0.9012 - val_auc: 0.9183 - val_loss: 0.2876 - val_pr_auc: 0.8576\n",
      "Epoch 38/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8903 - auc: 0.9334 - loss: 0.3090 - pr_auc: 0.8720 - val_accuracy: 0.9066 - val_auc: 0.9199 - val_loss: 0.2824 - val_pr_auc: 0.8606\n",
      "Epoch 39/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8959 - auc: 0.9324 - loss: 0.3086 - pr_auc: 0.8721 - val_accuracy: 0.9062 - val_auc: 0.9210 - val_loss: 0.2821 - val_pr_auc: 0.8613\n",
      "Epoch 40/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8949 - auc: 0.9311 - loss: 0.3097 - pr_auc: 0.8715 - val_accuracy: 0.9022 - val_auc: 0.9198 - val_loss: 0.2853 - val_pr_auc: 0.8599\n",
      "Epoch 41/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8938 - auc: 0.9341 - loss: 0.3061 - pr_auc: 0.8745 - val_accuracy: 0.9071 - val_auc: 0.9188 - val_loss: 0.2805 - val_pr_auc: 0.8576\n",
      "Epoch 42/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8921 - auc: 0.9341 - loss: 0.3059 - pr_auc: 0.8743 - val_accuracy: 0.9047 - val_auc: 0.9185 - val_loss: 0.2810 - val_pr_auc: 0.8570\n",
      "Epoch 43/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8957 - auc: 0.9332 - loss: 0.3076 - pr_auc: 0.8735 - val_accuracy: 0.8965 - val_auc: 0.9205 - val_loss: 0.2897 - val_pr_auc: 0.8611\n",
      "Epoch 44/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8927 - auc: 0.9320 - loss: 0.3079 - pr_auc: 0.8724 - val_accuracy: 0.9052 - val_auc: 0.9193 - val_loss: 0.2827 - val_pr_auc: 0.8575\n",
      "Epoch 45/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8960 - auc: 0.9332 - loss: 0.3061 - pr_auc: 0.8736 - val_accuracy: 0.9043 - val_auc: 0.9207 - val_loss: 0.2836 - val_pr_auc: 0.8600\n",
      "Epoch 46/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9014 - auc: 0.9360 - loss: 0.3012 - pr_auc: 0.8776 - val_accuracy: 0.8984 - val_auc: 0.9191 - val_loss: 0.2887 - val_pr_auc: 0.8589\n",
      "Epoch 47/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8920 - auc: 0.9336 - loss: 0.3061 - pr_auc: 0.8741 - val_accuracy: 0.9050 - val_auc: 0.9183 - val_loss: 0.2856 - val_pr_auc: 0.8588\n",
      "Epoch 48/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8961 - auc: 0.9338 - loss: 0.3041 - pr_auc: 0.8752 - val_accuracy: 0.9040 - val_auc: 0.9188 - val_loss: 0.2808 - val_pr_auc: 0.8589\n",
      "Epoch 49/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8942 - auc: 0.9339 - loss: 0.3060 - pr_auc: 0.8735 - val_accuracy: 0.8935 - val_auc: 0.9172 - val_loss: 0.2958 - val_pr_auc: 0.8565\n",
      "Epoch 50/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8966 - auc: 0.9356 - loss: 0.3015 - pr_auc: 0.8768 - val_accuracy: 0.8951 - val_auc: 0.9190 - val_loss: 0.2921 - val_pr_auc: 0.8599\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "ROC-AUC Score: 0.9189\n",
      "Confusion Matrix:\n",
      " [[3857  630]\n",
      " [ 250  991]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9391    0.8596    0.8976      4487\n",
      "         1.0     0.6114    0.7985    0.6925      1241\n",
      "\n",
      "    accuracy                         0.8464      5728\n",
      "   macro avg     0.7752    0.8291    0.7951      5728\n",
      "weighted avg     0.8681    0.8464    0.8532      5728\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pr_auc_ann' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 94\u001b[0m\n\u001b[0;32m     83\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_path)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Save model metadata\u001b[39;00m\n\u001b[0;32m     86\u001b[0m meta \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANN (simple)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: THRESHOLD,\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_units\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m],\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m],\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e-3\u001b[39m,\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: class_weight,\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_auc_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: roc_auc_ann,\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpr_auc_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: pr_auc_ann,\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score_positive_class\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1_ann,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBN+ReLU+Dropout; numeric-only scaled inputs; seed=42\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     97\u001b[0m }\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(meta_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     99\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(meta, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pr_auc_ann' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "\n",
    "\n",
    "\n",
    "X_train_np = np.asarray(X_train_scaled, dtype=np.float32)\n",
    "X_test_np  = np.asarray(X_test_scaled,  dtype=np.float32)\n",
    "y_train_np = np.asarray(y_train, dtype=np.float32)\n",
    "y_test_np  = np.asarray(y_test,  dtype=np.float32)\n",
    "\n",
    "classes = np.unique(y_train_np)\n",
    "cw_vals = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train_np)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, cw_vals)}\n",
    "print(\"Class weights:\", class_weight)\n",
    "\n",
    "def simple_ann_sequential(\n",
    "    input_dim: int,\n",
    "    hidden_units=(128, 64),\n",
    "    dropout=(0.2, 0.2),\n",
    "    lr=1e-3,\n",
    "):\n",
    "    model = models.Sequential(name=\"simple_ann\")\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "\n",
    "    # Hidden layers with Batch Normalization and Dropout\n",
    "    for units, dr in zip(hidden_units, dropout):\n",
    "        model.add(layers.Dense(units, activation=\"relu\"))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dr))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.AUC(name=\"auc\", curve=\"ROC\"),\n",
    "        ],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "THRESHOLD = 0.40\n",
    "input_dim = X_train_np.shape[1]\n",
    "model = simple_ann_sequential(input_dim=input_dim)\n",
    "#model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_np, y_train_np,\n",
    "    validation_data=(X_test_np, y_test_np),\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "def predict_with_threshold(probs, thr=THRESHOLD):\n",
    "    return (probs >= thr).astype(int)\n",
    "\n",
    "# Correctly evaluate and save the metrics to variables\n",
    "y_prob = model.predict(X_test_np).ravel()\n",
    "y_pred = predict_with_threshold(y_prob, thr=THRESHOLD)\n",
    "\n",
    "# Calculate and store the metrics\n",
    "roc_auc_ann = roc_auc_score(y_test_np, y_prob)\n",
    "f1_ann = f1_score(y_test_np, y_pred)\n",
    "cm_ann = confusion_matrix(y_test_np, y_pred)\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc_ann:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm_ann)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_np, y_pred, digits=4))\n",
    "print(\"-\" * 70)\n",
    "\n",
    "model_path = \"ann_model_simple.keras\"\n",
    "meta_path  = \"ann_meta_simple.json\"\n",
    "\n",
    "# Save model\n",
    "model.save(model_path)\n",
    "\n",
    "# Save model metadata\n",
    "meta = {\n",
    "    \"model_name\": \"ANN (simple)\",\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"hidden_units\": [128, 64],\n",
    "    \"dropout\": [0.2, 0.2],\n",
    "    \"lr\": 1e-3,\n",
    "    \"class_weight\": class_weight,\n",
    "    \"roc_auc_score\": roc_auc_ann,\n",
    "    \"f1_score_positive_class\": f1_ann,\n",
    "    \"notes\": \"BN+ReLU+Dropout; numeric-only scaled inputs; seed=42\",\n",
    "}\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"Saved: {model_path}, {meta_path}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 688532,
     "sourceId": 1207035,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
